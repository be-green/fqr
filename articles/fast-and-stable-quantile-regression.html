<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>A Simple Method for Fast and Accurate Quantile Regression • fqr</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="A Simple Method for Fast and Accurate Quantile Regression">
<meta property="og:description" content="fqr">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">fqr</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/fast-and-stable-quantile-regression.html">A Simple Method for Fast and Accurate Quantile Regression</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="fast-and-stable-quantile-regression_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>A Simple Method for Fast and Accurate Quantile Regression</h1>
            
      
      
      <div class="hidden name"><code>fast-and-stable-quantile-regression.Rmd</code></div>

    </div>

    
    
<div id="abstract" class="section level1">
<h1 class="hasAnchor">
<a href="#abstract" class="anchor"></a>Abstract</h1>
<p>Quantile regression is one of the most powerful statistical tools for studying conditional distributions. However, for large N problems, traditional linear programming tools are extremely slow. On the other hand first-order methods like semi-smooth and proximal gradient descent, and methods with similar convergence speed like ADMM, quickly come within a few digit accuracy, but have difficulty matching the precision of linear programming approaches. I address both of these problems, using a technique which first comes to an approximate solution, collapses the data into an efficient representation, and post-processes with linear programming approaches. This method converges to an exact answer quickly for large problems and can be easily applied to any first-order quantile regression method.</p>
</div>
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>Quantile regression is a fantastic tool for understanding richer information than simple averages. Rather than minimizing a least squares loss, quantile regression conveys the conditional quantile <span class="math inline">\(\tau\)</span> by targeting</p>
<p><span class="math display">\[\min_{b\in\mathbb{R}^p} \sum_{i \in N} \rho(y - x^Tb)\]</span></p>
<p>where for a given <span class="math inline">\(\tau\)</span>,</p>
<p><span class="math display">\[\rho(r) = r(\tau - I(r &gt; 0))\]</span> and <span class="math inline">\(I\)</span> is the indicator function.</p>
<p>However, for large problems existing methods for quantile regression are computationally infeasible. Traditional methods for minimizing this loss function involve formulating the problem as a linear program, and leveraging techniques from that literature such as those involving simplex representations <span class="citation">(Barrodale and Roberts 1973)</span> and interior point methods <span class="citation">(Portnoy and Koenker 1997)</span>. These are very efficient for small and medium scale problems, but are slow for problems with large numbers of observations.</p>
<p>There is increasing interest in using quantile regression with big N. For example, many economists are interested in studying income inequality with administrative data, but the computational tools which scale well with sample size can take a long time to achieve a high degree of accuracy. When trying to predict future outcomes with a linear quantile regression model, error in the individual coefficients is less important than the level of error in the predicted values. However, for inferential purposes the reverse can be true.</p>
<p><span class="citation">Koenker et al. (2017)</span> summarizes this nicely:</p>
<blockquote>
<p>In some applications it can be easily disregarded since decisions based on such data analysis only require a couple of digits accuracy. Nevertheless, it is somewhat disconcerting in view of our usual obsessions with rates of convergence of statistical procedures.</p>
</blockquote>
<p>In reported numerical experiments, using the alternating directional method of multipliers (ADMM) took 120,002 iterations to achieve 4-digit accuracy of its solution, but only took 1,421 iterations to achieve 2-digit accuracy.</p>
<p>To make matters worse, for joint models of quantiles which avoid crossings, as in <span class="citation">Schmidt and Zhu (2016)</span>, error in one quantile is be carried over into the next, and small instabilities are magnified exponentially when estimating the “spacings” away from an estimated median.</p>
<p>For variable selection, data-augmentation approaches to implementing a lasso penalty for quantile regression, such as those proposed in <span class="citation">Belloni and Chernozhukov (2011)</span>, rely on the solution sitting at a vertex of the linear program, mimicking the sharp geometry of the L1 penalty. Prominent implementations of the lasso penalty for quantile regression leverage this approach, for example in the quantreg <span class="citation">(Koenker et al. 2018)</span> and rqPen <span class="citation">(Sherwood et al. 2020)</span> R packages.</p>
<p>The key observation in this paper is quite simple, and is based on work by <span class="citation">Portnoy and Koenker (1997)</span>. The quantile regression loss function depends on whether the residuals of the predicted values change sign. Suppose we were estimating the conditional median. <span class="citation">Koenker (2005)</span> notes that if we “knew” that a certain subset <span class="math inline">\(J_h\)</span> of the observations fall above the optimal plane, and another <span class="math inline">\(J_l\)</span> were below the optimal plane, we could collapse them into single observations such that <span class="math inline">\(x_L = \sum_{i \in J_L} x_i\)</span>, <span class="math inline">\(y_L = \sum_{i \in J_L} y_i\)</span>, defining <span class="math inline">\(x_H\)</span> and <span class="math inline">\(y_h\)</span> similarly. Then we can estimate the revised problem:</p>
<p><span class="math display">\[\min_{b\in\mathbb{R}^p} \sum_{i \in N \ J_L \cup L_h} | y_i - x_i^T b | + |y_L - x_L^T b| + |y_H - x_H^T b|\]</span></p>
<p>Thus, if we have a high-quality guess of the true values for <span class="math inline">\(b\)</span>, we can identify a large number of observations whose residuals are not going to change sign when we move from <span class="math inline">\(b_{guess}\)</span> to <span class="math inline">\(b^*\)</span>.</p>
<p>This shrinks the effect <span class="math inline">\(N\)</span> of the problem by <span class="math inline">\(2J - 2\)</span>. In the vast majority of cases, 2 or 3 digit accuracy is enough to shrink the data enough for interior or exterior point methods to solve the problem quickly, getting an accurate solution in a few fast iterations.</p>
<p>There has been substantial recent growth in first-order, proximal, and smoothing-based methods for quantile regression including using ADMM <span class="citation">(Yu, Lin, and Wang 2017)</span>, smooth approximations of the loss function <span class="citation">(He et al. 2021)</span>, proximal gradient descent [reference I can’t find right now], and smoothed Newton coordinate-descent <span class="citation">(Yi and Huang 2017)</span>. These are substantially faster than interior point methods for large <span class="math inline">\(N\)</span> and <span class="math inline">\(P\)</span> because they do not require repeated cholesky factorization, which becomes slow with large numbers of observations.</p>
<p>This method can be applied to <em>any</em> solution method that comes within an <span class="math inline">\(\espilon\)</span>-ball of the optimal loss function, and can be used to post-process any of these approaches.</p>
</div>
<div id="the-algorithm" class="section level1">
<h1 class="hasAnchor">
<a href="#the-algorithm" class="anchor"></a>The Algorithm</h1>
</div>
<div id="numerical-experiments" class="section level1">
<h1 class="hasAnchor">
<a href="#numerical-experiments" class="anchor"></a>Numerical Experiments</h1>
<div id="convergence-speed-and-accuracy" class="section level2">
<h2 class="hasAnchor">
<a href="#convergence-speed-and-accuracy" class="anchor"></a>Convergence Speed and Accuracy</h2>
</div>
<div id="stabilizing-the-lasso" class="section level2">
<h2 class="hasAnchor">
<a href="#stabilizing-the-lasso" class="anchor"></a>Stabilizing the Lasso</h2>
</div>
</div>
<div id="empirical-implementation" class="section level1">
<h1 class="hasAnchor">
<a href="#empirical-implementation" class="anchor"></a>Empirical Implementation</h1>
</div>
<div id="conclusions" class="section level1">
<h1 class="hasAnchor">
<a href="#conclusions" class="anchor"></a>Conclusions</h1>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-barrodale1973improved">
<p>Barrodale, Ian, and Frank DK Roberts. 1973. “An Improved Algorithm for Discrete L_1 Linear Approximation.” <em>SIAM Journal on Numerical Analysis</em> 10 (5): 839–48.</p>
</div>
<div id="ref-belloni2011l1">
<p>Belloni, Alexandre, and Victor Chernozhukov. 2011. “ℓ1-Penalized Quantile Regression in High-Dimensional Sparse Models.” <em>The Annals of Statistics</em> 39 (1): 82–130.</p>
</div>
<div id="ref-he2021smoothed">
<p>He, Xuming, Xiaoou Pan, Kean Ming Tan, and Wen-Xin Zhou. 2021. “Smoothed Quantile Regression with Large-Scale Inference.” <em>Journal of Econometrics</em>.</p>
</div>
<div id="ref-koenker_2005">
<p>Koenker, Roger. 2005. <em>Quantile Regression</em>. Econometric Society Monographs. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511754098">https://doi.org/10.1017/CBO9780511754098</a>.</p>
</div>
<div id="ref-koenker2017handbook">
<p>Koenker, Roger, Victor Chernozhukov, Xuming He, and Limin Peng. 2017. “Handbook of Quantile Regression.”</p>
</div>
<div id="ref-koenker2018package">
<p>Koenker, Roger, Stephen Portnoy, Pin Tian Ng, Achim Zeileis, Philip Grosjean, and Brian D Ripley. 2018. “Package ‘Quantreg’.” <em>Cran R-Project. Org</em>.</p>
</div>
<div id="ref-portnoy1997gaussian">
<p>Portnoy, Stephen, and Roger Koenker. 1997. “The Gaussian Hare and the Laplacian Tortoise: Computability of Squared-Error Versus Absolute-Error Estimators.” <em>Statistical Science</em> 12 (4): 279–300.</p>
</div>
<div id="ref-schmidt2016quantile">
<p>Schmidt, Lawrence, and Yinchu Zhu. 2016. “Quantile Spacings: A Simple Method for the Joint Estimation of Multiple Quantiles Without Crossing.” <em>Available at SSRN 2220901</em>.</p>
</div>
<div id="ref-sherwood2020package">
<p>Sherwood, Ben, Adam Maidman, Maintainer Ben Sherwood, and TRUE ByteCompile. 2020. “Package ‘rqPen’.”</p>
</div>
<div id="ref-yi2017semismooth">
<p>Yi, Congrui, and Jian Huang. 2017. “Semismooth Newton Coordinate Descent Algorithm for Elastic-Net Penalized Huber Loss Regression and Quantile Regression.” <em>Journal of Computational and Graphical Statistics</em> 26 (3): 547–57.</p>
</div>
<div id="ref-yu2017parallel">
<p>Yu, Liqun, Nan Lin, and Lan Wang. 2017. “A Parallel Algorithm for Large-Scale Nonconvex Penalized Quantile Regression.” <em>Journal of Computational and Graphical Statistics</em> 26 (4): 935–39.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Brice Green.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
